
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>contrib.tpu.batch_parallel - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Defined in tensorflow&#47;contrib&#47;tpu&#47;python&#47;tpu&#47;tpu.py. ">
  <meta name="keywords" content="tf, contrib, tpu, batch, parallel, -, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/tensorflow~python/tf/contrib/tpu/batch_parallel/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-e0f881b0fce8cbe96f33dadc355b7159d5d2912911231446d60eb8f9caffa802.css">
  <script type="text/javascript" src="/assets/application-8fc46316434047265cd383f02b8e7f434ed7dec4a59f920dd633deef8d488d1b.js"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _tensorflow">
				
<h1 itemprop="name" class="devsite-page-title"> tf.contrib.tpu.batch_parallel </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.contrib.tpu.batch_parallel"> <meta itemprop="path" content="r1.4"> </div> <pre class="prettyprint lang-python" data-language="python">batch_parallel(
    computation,
    inputs=None,
    num_shards=1,
    infeed_queue=None,
    global_tpu_id=None,
    name=None
)
</pre> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/tpu/python/tpu/tpu.py" target="_blank"><code>tensorflow/contrib/tpu/python/tpu/tpu.py</code></a>.</p> <p>Shards <code>computation</code> along the batch dimension for parallel execution.</p> <p>Convenience wrapper around shard().</p> <p><code>inputs</code> must be a list of Tensors or None (equivalent to an empty list). Each input is split into <code>num_shards</code> pieces along the 0-th dimension, and computation is applied to each shard in parallel.</p> <p>Tensors are broadcast to all shards if they are lexically captured by <code>computation</code>. e.g.,</p> <p>x = tf.constant(7) def computation(): return x + 3 ... = shard(computation, ...)</p> <p>The outputs from all shards are concatenated back together along their 0-th dimension.</p> <p>Inputs and outputs of the computation must be at least rank-1 Tensors.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>computation</code></b>: a Python function that builds a computation to apply to each shard of the input.</li> <li>
<b><code>inputs</code></b>: a list of input tensors or None (equivalent to an empty list). The 0-th dimension of each Tensor must have size divisible by <code>num_shards</code>.</li> <li>
<b><code>num_shards</code></b>: the number of shards.</li> <li>
<b><code>infeed_queue</code></b>: if not None, the InfeedQueue from which to append a tuple of arguments as inputs to <code>computation</code>.</li> <li>
<b><code>global_tpu_id</code></b>: if not None, a Numpy 2D array indicating the global id of each TPU device in the system. The outer dimension of the array is host task id, and the inner dimension is device ordinal, so e.g., global_tpu_id[x][y] indicates the global id of device /task:x/device:TPU_NODE:y.</li> <li>
<b><code>name</code></b>: name of the operator.</li> </ul> <h4 id="returns">Returns:</h4> <p>A list of output tensors.</p> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: if num_shards &lt;= 0</li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2017 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/batch_parallel" class="_attribution-link" target="_blank">https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/batch_parallel</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
