
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>contrib.bayesflow.csiszar_divergence.monte_carlo_csiszar_f_divergence - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Defined in tensorflow&#47;contrib&#47;bayesflow&#47;python&#47;ops&#47;csiszar_divergence_impl.py. ">
  <meta name="keywords" content="tf, contrib, bayesflow, csiszar, divergence, monte, carlo, f, -, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/tensorflow~python/tf/contrib/bayesflow/csiszar_divergence/monte_carlo_csiszar_f_divergence/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-e0f881b0fce8cbe96f33dadc355b7159d5d2912911231446d60eb8f9caffa802.css">
  <script type="text/javascript" src="/assets/application-8fc46316434047265cd383f02b8e7f434ed7dec4a59f920dd633deef8d488d1b.js"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _tensorflow">
				
<h1 itemprop="name" class="devsite-page-title"> tf.contrib.bayesflow.csiszar_divergence.monte_carlo_csiszar_f_divergence </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.contrib.bayesflow.csiszar_divergence.monte_carlo_csiszar_f_divergence"> <meta itemprop="path" content="r1.4"> </div> <pre class="prettyprint lang-python" data-language="python">monte_carlo_csiszar_f_divergence(
    f,
    p_log_prob,
    q,
    num_draws,
    use_reparametrization=None,
    seed=None,
    name=None
)
</pre> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/bayesflow/python/ops/csiszar_divergence_impl.py" target="_blank"><code>tensorflow/contrib/bayesflow/python/ops/csiszar_divergence_impl.py</code></a>.</p> <p>Monte-Carlo approximation of the Csiszar f-Divergence.</p> <p>A Csiszar-function is a member of,</p> <pre class="prettyprint lang-none" data-language="python">F = { f:R_+ to R : f convex }.
</pre> <p>The Csiszar f-Divergence for Csiszar-function f is given by:</p> <pre class="prettyprint lang-none" data-language="python">D_f[p(X), q(X)] := E_{q(X)}[ f( p(X) / q(X) ) ]
                ~= m**-1 sum_j^m f( p(x_j) / q(x_j) ),
                           where x_j ~iid q(X)
</pre> <p>Tricks: Reparameterization and Score-Gradient</p> <p>When q is "reparameterized", i.e., a diffeomorphic transformation of a parameterless distribution (e.g., <code>Normal(Y; m, s) &lt;=&gt; Y = sX + m, X ~ Normal(0,1)</code>), we can swap gradient and expectation, i.e., <code>grad[Avg{ s_i : i=1...n }] = Avg{ grad[s_i] : i=1...n }</code> where <code>S_n=Avg{s_i}</code> and <code>s_i = f(x_i), x_i ~iid q(X)</code>.</p> <p>However, if q is not reparameterized, TensorFlow's gradient will be incorrect since the chain-rule stops at samples of unreparameterized distributions. In this circumstance using the Score-Gradient trick results in an unbiased gradient, i.e.,</p> <pre class="prettyprint lang-none" data-language="python">grad[ E_q[f(X)] ]
= grad[ int dx q(x) f(x) ]
= int dx grad[ q(x) f(x) ]
= int dx [ q'(x) f(x) + q(x) f'(x) ]
= int dx q(x) [q'(x) / q(x) f(x) + f'(x) ]
= int dx q(x) grad[ f(x) q(x) / stop_grad[q(x)] ]
= E_q[ grad[ f(x) q(x) / stop_grad[q(x)] ] ]
</pre> <p>Unless <code>q.reparameterization_type != distribution.FULLY_REPARAMETERIZED</code> it is usually preferable to set <code>use_reparametrization = True</code>.</p> <p>Example Application:</p> <p>The Csiszar f-Divergence is a useful framework for variational inference. I.e., observe that,</p> <pre class="prettyprint lang-none" data-language="python">f(p(x)) =  f( E_{q(Z | x)}[ p(x, Z) / q(Z | x) ] )
        &lt;= E_{q(Z | x)}[ f( p(x, Z) / q(Z | x) ) ]
        := D_f[p(x, Z), q(Z | x)]
</pre> <p>The inequality follows from the fact that the "perspective" of <code>f</code>, i.e., <code>(s, t) |-&gt; t f(s / t))</code>, is convex in <code>(s, t)</code> when <code>s/t in domain(f)</code> and <code>t</code> is a real. Since the above framework includes the popular Evidence Lower BOund (ELBO) as a special case, i.e., <code>f(u) = -log(u)</code>, we call this framework "Evidence Divergence Bound Optimization" (EDBO).</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>f</code></b>: Python <code>callable</code> representing a Csiszar-function in log-space, i.e., takes <code>p_log_prob(q_samples) - q.log_prob(q_samples)</code>.</li> <li>
<b><code>p_log_prob</code></b>: Python <code>callable</code> taking (a batch of) samples from <code>q</code> and returning the natural-log of the probability under distribution <code>p</code>. (In variational inference <code>p</code> is the joint distribution.)</li> <li>
<b><code>q</code></b>: <code>tf.Distribution</code>-like instance; must implement: <code>reparameterization_type</code>, <code>sample(n, seed)</code>, and <code>log_prob(x)</code>. (In variational inference <code>q</code> is the approximate posterior distribution.)</li> <li>
<b><code>num_draws</code></b>: Integer scalar number of draws used to approximate the f-Divergence expectation.</li> <li>
<b><code>use_reparametrization</code></b>: Python <code>bool</code>. When <code>None</code> (the default), automatically set to: <code>q.reparameterization_type == distribution.FULLY_REPARAMETERIZED</code>. When <code>True</code> uses the standard Monte-Carlo average. When <code>False</code> uses the score-gradient trick. (See above for details.) When <code>False</code>, consider using <code>csiszar_vimco</code>.</li> <li>
<b><code>seed</code></b>: Python <code>int</code> seed for <code>q.sample</code>.</li> <li>
<b><code>name</code></b>: Python <code>str</code> name prefixed to Ops created by this function.</li> </ul> <h4 id="returns">Returns:</h4> <ul> <li>
<b><code>monte_carlo_csiszar_f_divergence</code></b>: <code>float</code>-like <code>Tensor</code> Monte Carlo approximation of the Csiszar f-Divergence.</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: if <code>q</code> is not a reparameterized distribution and <code>use_reparametrization = True</code>. A distribution <code>q</code> is said to be "reparameterized" when its samples are generated by transforming the samples of another distribution which does not depend on the parameterization of <code>q</code>. This property ensures the gradient (with respect to parameters) is valid.</li> <li>
<b><code>TypeError</code></b>: if <code>p_log_prob</code> is not a Python <code>callable</code>.</li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2017 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/bayesflow/csiszar_divergence/monte_carlo_csiszar_f_divergence" class="_attribution-link" target="_blank">https://www.tensorflow.org/api_docs/python/tf/contrib/bayesflow/csiszar_divergence/monte_carlo_csiszar_f_divergence</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
